{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "(Distributed) training script for scene segmentation\n",
    "This file currently supports training and testing on S3DIS\n",
    "If more than 1 GPU is provided, will launch multi processing distributed training by default\n",
    "if you only wana use 1 GPU, set `CUDA_VISIBLE_DEVICES` accordingly\n",
    "\"\"\"\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import __init__\n",
    "import argparse, yaml, os, logging, numpy as np, csv, wandb, glob\n",
    "from tqdm import tqdm\n",
    "import torch, torch.nn as nn\n",
    "from torch import distributed as dist, multiprocessing as mp\n",
    "from openpoints.utils import set_random_seed, save_checkpoint, load_checkpoint, resume_checkpoint, setup_logger_dist, \\\n",
    "    cal_model_parm_nums, Wandb, generate_exp_directory, resume_exp_directory, EasyConfig, dist_utils, find_free_port\n",
    "from openpoints.utils import AverageMeter\n",
    "from openpoints.dataset import build_dataloader_from_cfg, get_features_by_keys, get_class_weights\n",
    "from openpoints.dataset.data_util import voxelize\n",
    "from openpoints.dataset.semantic_kitti.semantickitti import load_label_kitti, load_pc_kitti, remap_lut_read, remap_lut_write, get_semantickitti_file_list\n",
    "from openpoints.transforms import build_transforms_from_cfg\n",
    "from openpoints.optim import build_optimizer_from_cfg\n",
    "from openpoints.scheduler import build_scheduler_from_cfg\n",
    "from openpoints.loss import build_criterion_from_cfg\n",
    "from openpoints.models import build_model_from_cfg\n",
    "import warnings\n",
    "import shutil\n",
    "\n",
    "from visualize import from_sample, plot, binned_cm\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(gpu, cfg):\n",
    "    # if cfg.distributed:\n",
    "    #     if cfg.mp:\n",
    "    #         cfg.rank = gpu\n",
    "    #     dist.init_process_group(backend=cfg.dist_backend,\n",
    "    #                             init_method=cfg.dist_url,\n",
    "    #                             world_size=cfg.world_size,\n",
    "    #                             rank=cfg.rank)\n",
    "    #     dist.barrier()\n",
    "\n",
    "    if cfg.criterion_args.NAME.lower() == 'weightedmse':\n",
    "        cfg.criterion_args.bins = cfg.dataset.common.bins\n",
    "        cfg.criterion_args.min = cfg.datatransforms.kwargs.norm_min\n",
    "        cfg.criterion_args.max = 1\n",
    "        cfg.criterion_args.weights = [1] * (cfg.dataset.common.bins - 1) + [0.25]\n",
    "    \n",
    "    if cfg.criterion_args.NAME.lower() == 'deltaloss':\n",
    "        cfg.criterion_args.delta = 0.8\n",
    "        cfg.criterion_args.power = 2\n",
    "    \n",
    "    if cfg.criterion_args.NAME.lower() == 'reductionloss':\n",
    "        cfg.criterion_args.bins = cfg.dataset.common.bins\n",
    "        cfg.criterion_args.min = cfg.datatransforms.kwargs.norm_min\n",
    "        cfg.criterion_args.max = 1\n",
    "        cfg.criterion_args.reduction = 1\n",
    "\n",
    "\n",
    "    # logger\n",
    "    setup_logger_dist(cfg.log_path, cfg.rank, name=cfg.dataset.common.NAME)\n",
    "    \n",
    "    # if cfg.rank == 0:\n",
    "    #     #if not cfg.wandb.sweep:        \n",
    "    #     #Wandb.launch(cfg, cfg.wandb.use_wandb)\n",
    "    #     writer = SummaryWriter(log_dir=cfg.run_dir) if cfg.is_training else None\n",
    "    # else:\n",
    "    #     writer = None\n",
    "    set_random_seed(cfg.seed, deterministic=cfg.deterministic)\n",
    "    torch.backends.cudnn.enabled = False\n",
    "    \n",
    "    # ! Commented\n",
    "    #logging.info(cfg)\n",
    "\n",
    "    if cfg.model.get('in_channels', None) is None:\n",
    "        cfg.model.in_channels = cfg.model.encoder_args.in_channels\n",
    "    \n",
    "    model = build_model_from_cfg(cfg.model).to(cfg.rank)\n",
    "    \n",
    "    # random_data = torch.rand()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    model_size = cal_model_parm_nums(model)\n",
    "    \n",
    "    logging.info(f'Cfg parameters:')\n",
    "    logging.info(cfg)\n",
    "    \n",
    "    # logging.info(model)\n",
    "    logging.info('Number of params: %.4f M' % (model_size / 1e6))\n",
    "\n",
    "    if cfg.sync_bn:\n",
    "        model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model)\n",
    "        \n",
    "        # ! commented\n",
    "        # logging.info('Using Synchronized BatchNorm ...')\n",
    "    if cfg.distributed:\n",
    "        torch.cuda.set_device(gpu)\n",
    "        # model = nn.parallel.DistributedDataParallel(model.cuda(), device_ids=[cfg.rank], output_device=cfg.rank)\n",
    "        logging.info(f\"Model is using {cfg.world_size} gpus in DatalParallel mode!\")\n",
    "        model = nn.parallel.DataParallel(model.cuda())\n",
    "        \n",
    "        # ! commented\n",
    "        # logging.info('Using Distributed Data parallel ...')\n",
    "\n",
    "    \n",
    "    # optimizer & scheduler\n",
    "    optimizer = build_optimizer_from_cfg(model, lr=cfg.lr, **cfg.optimizer)    \n",
    "    scheduler = build_scheduler_from_cfg(cfg, optimizer)  \n",
    "    '''\n",
    "    # build dataset    \n",
    "    test_loader, test_histogram = build_dataloader_from_cfg(cfg.get('test_batch_size', cfg.batch_size),\n",
    "                                            cfg.dataset,\n",
    "                                            cfg.dataloader,\n",
    "                                            datatransforms_cfg=cfg.datatransforms,\n",
    "                                            split='test',\n",
    "                                            distributed=False\n",
    "                                            )    \n",
    "    '''    \n",
    "    # Save the model in the exchangeable ONNX format        \n",
    "    input_names = ['points', 'meta']\n",
    "    output_names = [\"output\", \"trans\"]\n",
    "        \n",
    "    # torch.onnx.disable_log()\n",
    "    # torch.onnx.export(model, next(iter(test_loader)), './onnx/model.onnx')\n",
    "    # sys.exit()\n",
    "\n",
    "    '''\n",
    "    # build dataset\n",
    "    val_loader, val_histogram = build_dataloader_from_cfg(cfg.get('val_batch_size', cfg.batch_size),\n",
    "                                            cfg.dataset,\n",
    "                                            cfg.dataloader,\n",
    "                                            datatransforms_cfg=cfg.datatransforms,\n",
    "                                            split='val',\n",
    "                                            distributed=False\n",
    "                                            )\n",
    "    \n",
    "    # ! commented\n",
    "    # logging.info(f\"length of validation dataset: {len(val_loader.dataset)}\")\n",
    "    num_classes = val_loader.dataset.num_classes if hasattr(val_loader.dataset, 'num_classes') else None\n",
    "    \n",
    "    if num_classes is not None:\n",
    "        assert cfg.num_classes == num_classes\n",
    "    \n",
    "    # ! commented\n",
    "    # logging.info(f\"number of classes of the dataset: {num_classes}\")\n",
    "    cfg.classes = val_loader.dataset.classes if hasattr(val_loader.dataset, 'classes') else np.arange(num_classes)\n",
    "    \n",
    "    cfg.cmap = np.array(val_loader.dataset.cmap) if hasattr(val_loader.dataset, 'cmap') else None\n",
    "    validate_fn = validate\n",
    "    \n",
    "    # optionally resume from a checkpoint\n",
    "    model_module = model.module if hasattr(model, 'module') else model\n",
    "    \n",
    "    if cfg.pretrained_path is not None:\n",
    "        if cfg.mode == 'resume':\n",
    "            resume_checkpoint(cfg, model, optimizer, scheduler, pretrained_path=cfg.pretrained_path)\n",
    "    else:\n",
    "        logging.info('Training from scratch')\n",
    "\n",
    "    if 'freeze_blocks' in cfg.mode:\n",
    "        for p in model_module.encoder.blocks.parameters():\n",
    "            p.requires_grad = False\n",
    "    '''\n",
    "    \n",
    "    train_loader, train_histogram = build_dataloader_from_cfg(cfg.batch_size,\n",
    "                                                cfg.dataset,\n",
    "                                                cfg.dataloader,\n",
    "                                                datatransforms_cfg=cfg.datatransforms,\n",
    "                                                split='train',\n",
    "                                                distributed=False,\n",
    "                                                )    \n",
    "\n",
    "    \n",
    "    # #HISTOGRAM\n",
    "    \n",
    "    # import matplotlib\n",
    "    # import matplotlib.pyplot as plt\n",
    "    # import math\n",
    "    # matplotlib.use('TkAgg')\n",
    "    # print(test_histogram)\n",
    "    # print(val_histogram)\n",
    "    # print(train_histogram)    \n",
    "    \n",
    "    # histogram = train_histogram + val_histogram + test_histogram\n",
    "    # print(histogram)\n",
    "    # sys.exit()\n",
    "    \n",
    "    # # Calculate bin edges\n",
    "    # bin_edges = torch.linspace(-1, 1, cfg.dataset.common.bins+1)\n",
    "    # print(f'{torch.sum(histogram)}')\n",
    "    \n",
    "    # #targets = (targets - dmin) / (dmax - dmin)\n",
    "    # bin_edges = ((bin_edges + 1) / 2) * 1000\n",
    "    # bin_edges = [int(math.ceil(edge)) for edge in bin_edges.tolist()]\n",
    "    # print(bin_edges)\n",
    "    # names = [f'[{bin_edges[i]} - {bin_edges[i+1]})' for i in range(len(bin_edges[:-1]))]\n",
    "\n",
    "    # # Plotting the histogram\n",
    "    # plt.bar(names, histogram)\n",
    "    # plt.xlabel('Solar Irradiance [kWh/m2]')\n",
    "    # plt.ylabel('Point frequency')\n",
    "    # plt.title('Solar irradiance distribution over points')\n",
    "    \n",
    "    # plt.show()\n",
    "    # sys.exit()\n",
    "    \n",
    "    \n",
    "    logging.info(f\"length of training dataset: {len(train_loader.dataset)}\")\n",
    "\n",
    "    if not cfg.regression:\n",
    "        cfg.criterion_args.weight = None\n",
    "        if cfg.get('cls_weighed_loss', False):\n",
    "            if hasattr(train_loader.dataset, 'num_per_class'):\n",
    "                cfg.criterion_args.weight = get_class_weights(train_loader.dataset.num_per_class, normalize=True)\n",
    "            else:\n",
    "                logging.info('`num_per_class` attribute is not founded in dataset')\n",
    "\n",
    "    if cfg.criterion_args.NAME.lower() == 'reductionloss':\n",
    "        cfg.criterion_args.histogram = train_histogram\n",
    "        \n",
    "        if train_histogram == None:\n",
    "            print(\"Reduction loss requires a valid train histogram, which is only available when dataset preprocessing is enabled.\")\n",
    "            raise RuntimeError\n",
    "    \n",
    "    \n",
    "    criterion = build_criterion_from_cfg(cfg.criterion_args).cuda()\n",
    "    \n",
    "    mse_criterion = torch.nn.MSELoss().cuda()\n",
    "    \n",
    "    # ===> start training\n",
    "    if cfg.use_amp:\n",
    "        scaler = torch.cuda.amp.GradScaler()\n",
    "    else:\n",
    "        scaler = None\n",
    "    best_val, best_epoch = float('inf'), 0\n",
    "    '''\n",
    "    test_array = iter(val_loader)\n",
    "    \n",
    "    evaluation_test_array_0 = next(test_array)\n",
    "    evaluation_test_array_1 = next(test_array)\n",
    "    evaluation_test_array_2 = next(test_array)\n",
    "    evaluation_test_array_3 = next(test_array)\n",
    "    evaluation_test_array_4 = next(test_array)\n",
    "    evaluation_train_array = next(iter(train_loader))\n",
    "    '''\n",
    "    total_iter = 0\n",
    "    \n",
    "    if cfg.wandb.use_wandb:\n",
    "        wandb.watch(model, criterion, log=\"parameters\", log_freq=1000)\n",
    "    '''\n",
    "    from_date = \"{:%Y_%m_%d_%H_%M_%S}\".format(datetime.now())\n",
    "    image_dir = f'.\\\\data\\\\images\\\\{cfg.cfg_basename}\\\\{from_date}\\\\'\n",
    "    \n",
    "    if not os.path.exists(image_dir + '\\\\evaluation'):\n",
    "        os.makedirs(image_dir + '\\\\evaluation')\n",
    "    \n",
    "    if not os.path.exists(image_dir + '\\\\training'):\n",
    "        os.makedirs(image_dir + '\\\\training')\n",
    "    \n",
    "    logging.info('Logging initial images...')\n",
    "    max_images = min([5, cfg.batch_size])\n",
    "    max_evaluation_images = 5\n",
    "    \n",
    "    for idx in range(max_images):\n",
    "        if idx == 0:\n",
    "            image_path_0 = eval_image(model, evaluation_test_array_0, idx, f'Epoch base test 0 sample {idx}', image_dir + '\\\\evaluation', cfg)\n",
    "            image_path_1 = eval_image(model, evaluation_test_array_1, idx, f'Epoch base test 1 sample {idx}', image_dir + '\\\\evaluation', cfg)\n",
    "            image_path_2 = eval_image(model, evaluation_test_array_2, idx, f'Epoch base test 2 sample {idx}', image_dir + '\\\\evaluation', cfg)\n",
    "            image_path_3 = eval_image(model, evaluation_test_array_3, idx, f'Epoch base test 3 sample {idx}', image_dir + '\\\\evaluation', cfg)\n",
    "            image_path_4 = eval_image(model, evaluation_test_array_4, idx, f'Epoch base test 4 sample {idx}', image_dir + '\\\\evaluation', cfg)\n",
    "            \n",
    "            if cfg.wandb.use_wandb:\n",
    "                wandb.log({f\"Evaluation Irradiance Predictions 0\": wandb.Image(image_path_0 + '.png')}, step=0)\n",
    "                wandb.log({f\"Evaluation Irradiance Predictions 1\": wandb.Image(image_path_1 + '.png')}, step=0)\n",
    "                wandb.log({f\"Evaluation Irradiance Predictions 2\": wandb.Image(image_path_2 + '.png')}, step=0)\n",
    "                wandb.log({f\"Evaluation Irradiance Predictions 3\": wandb.Image(image_path_3 + '.png')}, step=0)\n",
    "                wandb.log({f\"Evaluation Irradiance Predictions 4\": wandb.Image(image_path_4 + '.png')}, step=0)\n",
    "        image_path = eval_image(model, evaluation_train_array, idx, f'Epoch 0 train sample {idx}', image_dir + '\\\\training', cfg)\n",
    "        \n",
    "        if cfg.wandb.use_wandb:\n",
    "            wandb.log({f\"Train Irradiance Predictions {idx}\": wandb.Image(image_path + '.png')}, step=0)\n",
    "    if cfg.wandb.use_wandb:\n",
    "        wandb.log({'crit': str(cfg.criterion_args.NAME)}, step=0)\n",
    "        wandb.log({'model': str(cfg.cfg_basename)}, step=0)\n",
    "        wandb.log({'optim': str(cfg.optimizer.NAME)}, step=0)\n",
    "        wandb.log({'sched': str(cfg.sched)}, step=0)\n",
    "        wandb.log({'batchsize': cfg.batch_size}, step=0)\n",
    "    '''\n",
    "    logging.info(f'Started training {cfg.cfg_basename} with criterion {cfg.criterion_args.NAME}, voxelsize {cfg.dataset.train.voxel_max}, batchsize {cfg.batch_size}...')\n",
    "    for epoch in range(cfg.start_epoch, cfg.epochs + 1):\n",
    "        # # ! Only important for distributed gpu\n",
    "        # if cfg.distributed:\n",
    "        #     train_loader.sampler.set_epoch(epoch)\n",
    "        # if hasattr(train_loader.dataset, 'epoch'):  # some dataset sets the dataset length as a fixed steps.\n",
    "        #     train_loader.dataset.epoch = epoch - 1\n",
    "        train_loss, train_rmse, total_iter = \\\n",
    "            train_one_epoch(model, train_loader, criterion, mse_criterion, optimizer, scheduler, scaler, epoch, total_iter, cfg)\n",
    "        sys.exit()\n",
    "        # ! Log the results from the epoch step\n",
    "        is_best = False\n",
    "        \n",
    "        logging.info(f\"Started evalution epoch {epoch}\")\n",
    "        if epoch % cfg.val_freq == 0:\n",
    "            \n",
    "            eval_loss, eval_rmse = validate_fn(model, val_loader, criterion, mse_criterion, cfg, epoch=epoch, total_iter=total_iter, image_dir=image_dir)\n",
    "            \n",
    "            if eval_loss < best_val:\n",
    "                logging.info(\"Found new best model!\")\n",
    "                is_best = True\n",
    "                best_val = eval_loss\n",
    "            \n",
    "        # ! Log to the writer\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        logging.info(f'Logging images for epoch {epoch}')\n",
    "        \n",
    "        max_images = min([5, cfg.batch_size])\n",
    "        for idx in range(max_images):\n",
    "            if idx == 0:\n",
    "                image_path_0 = eval_image(model, evaluation_test_array_0, idx, f'Epoch {epoch} test 0 sample {idx}', image_dir + '\\\\evaluation', cfg)\n",
    "                image_path_1 = eval_image(model, evaluation_test_array_1, idx, f'Epoch {epoch} test 1 sample {idx}', image_dir + '\\\\evaluation', cfg)\n",
    "                image_path_2 = eval_image(model, evaluation_test_array_2, idx, f'Epoch {epoch} test 2 sample {idx}', image_dir + '\\\\evaluation', cfg)\n",
    "                image_path_3 = eval_image(model, evaluation_test_array_3, idx, f'Epoch {epoch} test 3 sample {idx}', image_dir + '\\\\evaluation', cfg)\n",
    "                image_path_4 = eval_image(model, evaluation_test_array_4, idx, f'Epoch {epoch} test 4 sample {idx}', image_dir + '\\\\evaluation', cfg)\n",
    "                \n",
    "                if cfg.wandb.use_wandb:\n",
    "                    wandb.log({f\"Evaluation Irradiance Predictions 0\": wandb.Image(image_path_0 + '.png')})\n",
    "                    wandb.log({f\"Evaluation Irradiance Predictions 1\": wandb.Image(image_path_1 + '.png')})\n",
    "                    wandb.log({f\"Evaluation Irradiance Predictions 2\": wandb.Image(image_path_2 + '.png')})\n",
    "                    wandb.log({f\"Evaluation Irradiance Predictions 3\": wandb.Image(image_path_3 + '.png')})\n",
    "                    wandb.log({f\"Evaluation Irradiance Predictions 4\": wandb.Image(image_path_4 + '.png')})\n",
    "            image_path = eval_image(model, evaluation_train_array, idx, f'Epoch {epoch} train sample {idx}', image_dir + '\\\\training', cfg)\n",
    "            \n",
    "            if cfg.wandb.use_wandb:\n",
    "                wandb.log({f\"Train Irradiance Predictions {idx}\": wandb.Image(image_path + '.png')})\n",
    "    \n",
    "        if epoch % cfg.val_freq == 0:\n",
    "            logging.info(f'Epoch {epoch} LR {lr:.6f} '\n",
    "                     f'train loss {train_loss:.2f}, eval loss {eval_loss:.2f}')\n",
    "        else:\n",
    "            logging.info(f'Epoch {epoch} LR {lr:.6f} '\n",
    "                     f'train loss {train_loss:.2f}')\n",
    "                  \n",
    "        if epoch % cfg.val_freq == 0:\n",
    "            wandb.log({'Evaluation Loss (mse)': eval_loss})\n",
    "            wandb.log({'Evaluation Loss (rmse) [kWh/m2]': eval_rmse})\n",
    "        \n",
    "        # writer.add_scalar('train_loss', train_loss, epoch)\n",
    "        # writer.add_scalar('RMSE per train step [kWh/m2]', train_rmse, epoch)\n",
    "        wandb.log({'Learning Rate': lr})\n",
    "         # ! Update the optimizer with scheduler\n",
    "        if cfg.sched_on_epoch:\n",
    "            if cfg.sched.lower() != 'plateau':\n",
    "                scheduler.step(epoch)\n",
    "            else:\n",
    "                scheduler.step(epoch, metric=train_loss)\n",
    "         # ! Save model parameters to file\n",
    "        if cfg.rank == 0:\n",
    "            save_checkpoint(cfg, model, epoch, optimizer, scheduler,\n",
    "                            additioanl_dict={'best_val': best_val},\n",
    "                            is_best=is_best,\n",
    "                            post_fix=f'ckpt_epoch_{epoch}'\n",
    "                            )\n",
    "            is_best = False\n",
    "        \n",
    "        if epoch == cfg.max_epoch:\n",
    "            logging.info('Early finish!')\n",
    "            break\n",
    "    \n",
    "    # Test the model using the test dataset\n",
    "    test(cfg, model, test_loader, image_dir=image_dir)\n",
    "    \n",
    "    wandb.finish(exit_code=True)\n",
    "\n",
    "def standardize(logits, targets, cfg):\n",
    "    dmin = cfg.datatransforms.kwargs.irradiance_min\n",
    "    dmax = 1\n",
    "    \n",
    "    if dmin == None: dmin = -1\n",
    "    \n",
    "    # Standardize logits\n",
    "    logits = (logits - dmin) / (dmax - dmin)\n",
    "    targets = (targets - dmin) / (dmax - dmin)\n",
    "\n",
    "    # Return in range [0, 1]\n",
    "    return logits, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, criterion, mse_criterion, optimizer, scheduler, scaler, epoch, total_iter, cfg):\n",
    "    loss_meter = AverageMeter()\n",
    "    rmse_meter = AverageMeter()\n",
    "     \n",
    "    model.train()  # set model to training mode\n",
    "    pbar = tqdm(enumerate(train_loader), total=train_loader.__len__(), desc='Train')\n",
    "\n",
    "    '''\n",
    "    individual_criterion = nn.MSELoss(reduction='none')\n",
    "    '''\n",
    "    \n",
    "    num_iter = 0\n",
    "    loss = torch.Tensor([0.0])\n",
    "    rmse = torch.Tensor([0.0])\n",
    "    mse_loss = torch.Tensor([0.0])\n",
    "    \n",
    "    for idx, data in pbar:\n",
    "        pbar.set_description(f\"TRAINING --- Average loss: {format(round(loss_meter.avg, 4), '.4f')}, Average RMSE: {format(round(rmse_meter.avg, 4), '.4f')} [kWh/m2], Loss: {round(loss.item(), 4)}, MSE: {round(mse_loss.item(), 4)}, RMSE: {round(rmse.item(), 4)} [kWh/m2]\")\n",
    "        pbar.refresh()\n",
    "        \n",
    "        keys = data.keys() if callable(data.keys) else data.keys\n",
    "        \n",
    "        # ! Send all data to GPU\n",
    "        for key in keys:\n",
    "            data[key] = data[key].cuda(non_blocking=True)\n",
    "\n",
    "        num_iter += 1\n",
    "        \n",
    "        target = data['y'].squeeze(-1)\n",
    "\n",
    "        \"\"\" debug\n",
    "        from openpoints.dataset import vis_points\n",
    "        vis_points(data['pos'].cpu().numpy()[0], labels=data['y'].cpu().numpy()[0])\n",
    "        vis_points(data['pos'].cpu().numpy()[0], data['x'][0, :3, :].transpose(1, 0))\n",
    "        end of debug \"\"\"\n",
    "        # ! Combine all the feature, so pos and heigts are combined to shape (batchsize, channels, points) (32, 4, 24000)\n",
    "        data['x'] = get_features_by_keys(data, cfg.feature_keys)\n",
    "        \n",
    "        # ! Overwrite data['x'] with number of model channgels\n",
    "        data['x'] = data['x'][:,:cfg.model.in_channels,:]\n",
    "        \n",
    "        # ! Set the epoch in the data\n",
    "        data['epoch'] = epoch\n",
    "        \n",
    "        total_iter += 1 \n",
    "        \n",
    "        # ! Set the iteration number in the data\n",
    "        data['iter'] = total_iter \n",
    "              \n",
    "        with torch.cuda.amp.autocast(enabled=cfg.use_amp):\n",
    "            print(data.shape)\n",
    "            sys.exit()\n",
    "            \n",
    "            # ! Cast all the data to the model\n",
    "            logits = model(data)\n",
    "            # print(f'max logits: {round(torch.max(logits).item(), 3)} max targets: {round(torch.max(target).item(),3)}')\n",
    "            # print(f'min logits {round(torch.min(logits).item(), 3)} min targets: {round(torch.min(target).item(), 3)}')\n",
    "        \n",
    "            logits = logits.squeeze(1)\n",
    "                       \n",
    "            '''\n",
    "            loss is used for backwards pass\n",
    "            mse_loss is used for performance comparison\n",
    "            '''\n",
    "            # Standardize the logits and targets to [0, 1]\n",
    "            # logits, target = standardize(logits, target, cfg)\n",
    "                 \n",
    "            if cfg.criterion_args.NAME.lower() == 'weightedmse' or cfg.criterion_args.NAME.lower() == 'reductionloss':\n",
    "                loss = criterion(logits, target, bins=data['bins'])\n",
    "            elif 'mask' in cfg.criterion_args.NAME.lower():\n",
    "                loss = criterion(logits, target, data['mask'])\n",
    "            else:\n",
    "                loss = criterion(logits, target)\n",
    "             \n",
    "            mse_loss = mse_criterion(logits, target)\n",
    "            \n",
    "        wandb.log({'Train Loss (non-MSE)': loss})\n",
    "        wandb.log({'Train Loss MSE': mse_loss})\n",
    "            \n",
    "        # Compute RMSE with real units\n",
    "        if cfg.regression:\n",
    "            logits, target = standardize(logits, target, cfg)\n",
    "            \n",
    "            with torch.cuda.amp.autocast(enabled=cfg.use_amp):\n",
    "                rmse = torch.sqrt(mse_criterion(logits * 1000, target * 1000))\n",
    "            rmse_meter.update(rmse.item())\n",
    "            \n",
    "            wandb.log({'Train Loss RMSE [kWh/m2]': rmse})\n",
    "            '''\n",
    "            individual_losses = torch.mean(individual_criterion(logits, target), 1)\n",
    "            \n",
    "            for j, (ind, index) in enumerate(zip(individual_losses, data['idx'])):\n",
    "                writer.add_scalar('mse_per_train_sample', ind, total_iter * cfg.batch_size + j)\n",
    "                writer.add_scalar('sample_idx', index, total_iter * cfg.batch_size + j)\n",
    "                '''\n",
    "        if cfg.use_amp:\n",
    "            scaler.scale(loss).backward()\n",
    "        else:\n",
    "            loss.backward()\n",
    "        \n",
    "        # optimize\n",
    "        if num_iter == cfg.step_per_update:\n",
    "            if cfg.get('grad_norm_clip') is not None and cfg.grad_norm_clip > 0.:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_norm_clip, norm_type=2)\n",
    "            num_iter = 0\n",
    "\n",
    "            if cfg.use_amp:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                optimizer.step()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            if not cfg.sched_on_epoch:\n",
    "                if cfg.sched != 'plateau':\n",
    "                    scheduler.step(epoch)\n",
    "                else:\n",
    "                    scheduler.step(epoch, metric=loss)\n",
    "            \n",
    "            # mem = torch.cuda.max_memory_allocated() / 1024. / 1024.\n",
    "            # print(f\"Memory after backward is {mem}\")\n",
    "        \n",
    "        loss_meter.update(loss.item())      \n",
    "    \n",
    "    return loss_meter.avg, rmse_meter.avg, total_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def config_to_cfg(config):\n",
    "    cfg = EasyConfig()    \n",
    "    cfg.update(config.cfg)\n",
    "    \n",
    "    '''\n",
    "    Special sweep parameters\n",
    "    '''\n",
    "    for key in config.keys():\n",
    "        if key == 'cfg':\n",
    "            pass\n",
    "        elif key == 'crit':\n",
    "            cfg.criterion_args.NAME = config[key]   \n",
    "            \n",
    "            if config[key].lower() == 'weightedmse':\n",
    "                cfg.criterion_args.bins = cfg.dataset.common.bins\n",
    "                cfg.criterion_args.min = cfg.datatransforms.kwargs.norm_min\n",
    "                cfg.criterion_args.max = 1\n",
    "                cfg.criterion_args.weights = [1] * (cfg.dataset.common.bins - 1) + [0.25]\n",
    "            \n",
    "            if config[key].lower() == 'deltaloss':\n",
    "                cfg.criterion_args.delta = 0.8\n",
    "                cfg.criterion_args.power = 2\n",
    "            \n",
    "            if config[key].lower() == 'reductionloss':\n",
    "                cfg.criterion_args.bins = cfg.dataset.common.bins\n",
    "                cfg.criterion_args.min = cfg.datatransforms.kwargs.norm_min\n",
    "                cfg.criterion_args.max = 1\n",
    "                cfg.criterion_args.reduction = 1\n",
    "            \n",
    "        elif key == 'optim':\n",
    "            cfg.optimizer.NAME = config[key]\n",
    "        elif key == 'voxel_max':\n",
    "            cfg.dataset.train.voxel_max = config[key]\n",
    "        else:\n",
    "            cfg[key] = config[key]\n",
    "        \n",
    "    return cfg\n",
    "\n",
    "def sweep_run(config=None):\n",
    "    # tell wandb to get started\n",
    "    with wandb.init(mode=\"disabled\", project=\"IrradianceNet_loss_sweep\", config=config, allow_val_change=True):        \n",
    "        # access all HPs through wandb.config, so logging matches execution!\n",
    "        config = wandb.config\n",
    "        \n",
    "        cfg = config_to_cfg(config)\n",
    "        \n",
    "        # multi processing.\n",
    "        if cfg['mp']:\n",
    "            port = find_free_port()\n",
    "            cfg['dist_url'] = f\"tcp://localhost:{port}\"\n",
    "            print('using mp spawn for distributed training')\n",
    "            mp.spawn(main, nprocs=cfg['world_size'], args=(cfg,))\n",
    "        else:\n",
    "            main(0, cfg)\n",
    "\n",
    "def sweep(cfg):\n",
    "    # Random initialization of arguments\n",
    "    sweep_config = {\n",
    "        'method': 'grid'\n",
    "        }\n",
    "    \n",
    "    # Metric\n",
    "    metric = {\n",
    "        'name': 'mse_loss',\n",
    "        'goal': 'minimize'   \n",
    "        }\n",
    "\n",
    "    sweep_config['metric'] = metric\n",
    "        \n",
    "    parameters_dict = {\n",
    "        'voxel_max': {\n",
    "            'values': [10000, 20000]\n",
    "            },\n",
    "        'crit': {\n",
    "            'values': ['WeightedMSE', 'DeltaLoss', 'ReductionLoss', 'MSELoss']\n",
    "        }    \n",
    "    }\n",
    "    \n",
    "    # ['plateau_lr', 'cosine_lr', 'tanh_lr', 'poly_lr']\n",
    "    parameters_dict.update({\n",
    "                'cfg': {'value': cfg}\n",
    "                })\n",
    "    \n",
    "    sweep_config['parameters'] = parameters_dict\n",
    "   \n",
    "    project = \"IrradianceNet_loss_sweep\"\n",
    "    sweep_id = wandb.sweep(sweep_config, project=project)\n",
    "    \n",
    "    wandb.agent(sweep_id, sweep_run, count=50, project=project)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser('Scene segmentation training/testing')\n",
    "    parser.add_argument('--cfg', type=str, required=False, default='cfgs/irradiance/irradiancenet-l.yaml', help='config file')\n",
    "    parser.add_argument('--profile', action='store_true', default=False, help='set to True to profile speed')\n",
    "    parser.add_argument('--sweep', required=False, action='store_true', default=False, help='set to True to profile speed')\n",
    "    args, opts = parser.parse_known_args()       \n",
    "        \n",
    "    name = sys.argv[2].split(\"/\")[2][:-5] + \"_\" + \"_\".join(sys.argv[3:][1::2])\n",
    "    cfg = EasyConfig()\n",
    "    \n",
    "    cfg.load(args.cfg, recursive=True)\n",
    "    cfg.update(opts)  # overwrite the default arguments in yml\n",
    "\n",
    "    if args.sweep:\n",
    "        cfg.wandb.sweep = True\n",
    "\n",
    "    if cfg.seed is None:\n",
    "        cfg.seed = np.random.randint(1, 10000)\n",
    "\n",
    "    # init distributed env first, since logger depends on the dist info.\n",
    "    cfg.rank, cfg.world_size, cfg.distributed, cfg.mp = dist_utils.get_dist_info(cfg)\n",
    "    # cfg.sync_bn = cfg.world_size > 1\n",
    "\n",
    "    # init log dir\n",
    "    cfg.task_name = args.cfg.split('.')[-2].split('/')[-2]  # task/dataset name, \\eg s3dis, modelnet40_cls\n",
    "    cfg.cfg_basename = args.cfg.split('.')[-2].split('/')[-1]  # cfg_basename, \\eg pointnext-xl\n",
    "    tags = [\n",
    "        'IrradianceNet',\n",
    "        cfg.task_name,  # task name (the folder of name under ./cfgs\n",
    "        cfg.mode\n",
    "    ]\n",
    "    \n",
    "    opt_list = [] # for checking experiment configs from logging file\n",
    "    for i, opt in enumerate(opts):\n",
    "        if 'rank' not in opt and 'dir' not in opt and 'root' not in opt and 'pretrain' not in opt and 'path' not in opt and 'wandb' not in opt and '/' not in opt:\n",
    "            opt_list.append(opt)\n",
    "    cfg.root_dir = os.path.join(cfg.root_dir, cfg.task_name)\n",
    "    cfg.opts = '-'.join(opt_list)\n",
    "\n",
    "    cfg.is_training = cfg.mode not in ['test', 'testing', 'val', 'eval', 'evaluation']\n",
    "    \n",
    "    if cfg.mode in ['resume', 'val', 'test']:\n",
    "        resume_exp_directory(cfg, pretrained_path=cfg.pretrained_path)\n",
    "        cfg.wandb.tags = [cfg.mode]\n",
    "    else:\n",
    "        generate_exp_directory(cfg, tags, additional_id=os.environ.get('MASTER_PORT', None))\n",
    "        cfg.wandb.tags = tags\n",
    "    \n",
    "    if not os.path.exists('.\\log\\logs'):\n",
    "        os.makedirs('.\\log\\logs')\n",
    "    \n",
    "    os.environ[\"JOB_LOG_DIR\"] = cfg.log_dir\n",
    "    \n",
    "    cfg_path = os.path.join(cfg.run_dir, \"cfg.yaml\")\n",
    "    with open(cfg_path, 'w') as f:\n",
    "        yaml.dump(cfg, f, indent=2)\n",
    "        \n",
    "        # ! check if system is Windows, copy cfg to log\n",
    "        if os.name == 'nt':\n",
    "            shutil.copy(args.cfg, cfg.run_dir)\n",
    "        else:\n",
    "            os.system('cp %s %s' % (args.cfg, cfg.run_dir))\n",
    "\n",
    "    cfg.cfg_path = cfg_path\n",
    "    # wandb config\n",
    "    cfg.wandb.name = cfg.run_name\n",
    "\n",
    "    if cfg.wandb.use_wandb:\n",
    "        wandb.login()\n",
    "    \n",
    "    cfg.mp = False\n",
    "    if cfg.wandb.sweep:\n",
    "        sweep(cfg)\n",
    "    else:\n",
    "        with wandb.init(mode=\"disabled\", project=\"Thesis_main_TEST\", name=name):\n",
    "            # multi processing\n",
    "            if cfg.mp:\n",
    "                port = find_free_port()\n",
    "                cfg.dist_url = f\"tcp://localhost:{port}\"\n",
    "                print('using mp spawn for distributed training')\n",
    "                mp.spawn(main, nprocs=cfg.world_size, args=(cfg,))\n",
    "            else:\n",
    "                main(0, cfg)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
